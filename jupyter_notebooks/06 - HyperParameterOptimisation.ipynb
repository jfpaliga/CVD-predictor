{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Hyper Parameter Optimisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Document hyperparameter selection and tuning strategies\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* outputs/datasets/collection/HeartDiseasePrediction.csv\n",
        "* Instructions on data cleaning and feature engineering from the relevant notebooks\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* There are no outputs from this notebook as it is solely for demonstration purposes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the raw dataset and replace values of 0 in RestingBP and Cholesterol with NaN ready for the ML pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"outputs/datasets/collection/HeartDiseasePrediction.csv\")\n",
        "\n",
        "for col in [\"RestingBP\", \"Cholesterol\"]:\n",
        "    df[col] = df[col].replace(0, np.nan)\n",
        "\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# ML Pipelines and Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline for Data Cleaning and Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Data Cleaning\n",
        "from feature_engine.imputation import MeanMedianImputer, RandomSampleImputer\n",
        "\n",
        "# Feature Engineering\n",
        "from feature_engine.discretisation import ArbitraryDiscretiser\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "\n",
        "\n",
        "def DataCleaningandFeatEngPipeline():\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"median_imputation\", MeanMedianImputer(imputation_method=\"median\",\n",
        "                                                variables=[\"RestingBP\"])),\n",
        "        (\"random_sample_imputation\", RandomSampleImputer(random_state=1,\n",
        "                                                         seed='general',\n",
        "                                                         variables=[\"Cholesterol\"])),\n",
        "        (\"arbitrary_discretisation\", ArbitraryDiscretiser(binning_dict={\"Oldpeak\":[-np.inf, 0, 1.5, np.inf]})),\n",
        "        (\"ordinal_encoding\", OrdinalEncoder(encoding_method=\"arbitrary\",\n",
        "                                            variables=[\"Sex\",\n",
        "                                                       \"ChestPainType\",\n",
        "                                                       \"FastingBS\",\n",
        "                                                       \"RestingECG\",\n",
        "                                                       \"ExerciseAngina\",\n",
        "                                                       \"ST_Slope\"])),\n",
        "        ])\n",
        "\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline for Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature Selection\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# ML Algorithms\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "def ClassificationPipeline(model):\n",
        "\n",
        "    pipeline = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"feat_selection\", SelectFromModel(model)),\n",
        "        (\"model\", model),\n",
        "        ])\n",
        "\n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Data into Train and Test Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The data is split into train and test sets and transformed using the data cleaning and feature engineering pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop([\"HeartDisease\"], axis=1),\n",
        "    df[\"HeartDisease\"],\n",
        "    test_size=0.2,\n",
        "    random_state=0,\n",
        ")\n",
        "\n",
        "data_cleaning_feat_eng_pipeline = DataCleaningandFeatEngPipeline()\n",
        "X_train = data_cleaning_feat_eng_pipeline.fit_transform(X_train)\n",
        "X_test = data_cleaning_feat_eng_pipeline.transform(X_test)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Target balancing is then carried out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "oversample = SMOTE(sampling_strategy='minority', random_state=0)\n",
        "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Load custom hyperparameter optimsation function from Code Institute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "class HyperparameterOptimizationSearch:\n",
        "\n",
        "    def __init__(self, models, params):\n",
        "        self.models = models\n",
        "        self.params = params\n",
        "        self.keys = models.keys()\n",
        "        self.grid_searches = {}\n",
        "\n",
        "    def fit(self, X, y, cv, n_jobs, verbose=1, scoring=None, refit=False):\n",
        "        for key in self.keys:\n",
        "            print(f\"\\nRunning GridSearchCV for {key} \\n\")\n",
        "\n",
        "            model = ClassificationPipeline(self.models[key])\n",
        "            params = self.params[key]\n",
        "            gs = GridSearchCV(model, params, cv=cv, n_jobs=n_jobs,\n",
        "                              verbose=verbose, scoring=scoring, )\n",
        "            gs.fit(X, y)\n",
        "            self.grid_searches[key] = gs\n",
        "\n",
        "    def score_summary(self, sort_by='mean_score'):\n",
        "        def row(key, scores, params):\n",
        "            d = {\n",
        "                'estimator': key,\n",
        "                'min_score': min(scores),\n",
        "                'max_score': max(scores),\n",
        "                'mean_score': np.mean(scores),\n",
        "                'std_score': np.std(scores),\n",
        "            }\n",
        "            return pd.Series({**params, **d})\n",
        "\n",
        "        rows = []\n",
        "        for k in self.grid_searches:\n",
        "            params = self.grid_searches[k].cv_results_['params']\n",
        "            scores = []\n",
        "            for i in range(self.grid_searches[k].cv):\n",
        "                key = \"split{}_test_score\".format(i)\n",
        "                r = self.grid_searches[k].cv_results_[key]\n",
        "                scores.append(r.reshape(len(params), 1))\n",
        "\n",
        "            all_scores = np.hstack(scores)\n",
        "            for p, s in zip(params, all_scores):\n",
        "                rows.append((row(k, s, p)))\n",
        "\n",
        "        df = pd.concat(rows, axis=1).T.sort_values([sort_by], ascending=False)\n",
        "        columns = ['estimator', 'min_score',\n",
        "                   'mean_score', 'max_score', 'std_score']\n",
        "        columns = columns + [c for c in df.columns if c not in columns]\n",
        "        return df[columns], self.grid_searches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Load custom model evaluation function from Code Institute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def confusion_matrix_and_report(X, y, pipeline, label_map):\n",
        "\n",
        "    prediction = pipeline.predict(X)\n",
        "\n",
        "    print('---  Confusion Matrix  ---')\n",
        "    print(pd.DataFrame(confusion_matrix(y_true=prediction, y_pred=y),\n",
        "          columns=[[\"Actual \" + sub for sub in label_map]],\n",
        "          index=[[\"Prediction \" + sub for sub in label_map]]\n",
        "          ))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print('---  Classification Report  ---')\n",
        "    print(classification_report(y, prediction, target_names=label_map), \"\\n\")\n",
        "\n",
        "\n",
        "def clf_performance(X_train, y_train, X_test, y_test, pipeline, label_map):\n",
        "    print(\"#### Train Set #### \\n\")\n",
        "    confusion_matrix_and_report(X_train, y_train, pipeline, label_map)\n",
        "\n",
        "    print(\"#### Test Set ####\\n\")\n",
        "    confusion_matrix_and_report(X_test, y_test, pipeline, label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to streamline the tuning and selection of hyperparameters, each algorithm was investigated and optimised individually before being tested against each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = {\"RandomForestClassifier\": RandomForestClassifier(random_state=0)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before carrying out the optimisation, it is best to understand what the hyperparameters to investigate mean:\n",
        "\n",
        "* ```n_estimators``` - The number of decision trees in the 'forest'.\n",
        "* ```max_depth``` - The maximum depth of each tree, ie the depth at which nodes will expand to until either all leaves contain only a single class or  all leaves contain less than the minimum number of sample required to split.\n",
        "* ```min_samples_split``` - The minimum number of samples required to split a node.\n",
        "* ```min_samples_leaf``` - The minimum number of samples required to be at a leaf node.\n",
        "* ```max_leaf_nodes``` - Trees with max_leaf_nodes are grown in best-first fashion.\n",
        "* ```max_features``` - The number of features to consider when looking for the best split. If None, then max_features is equal to the total number of features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first round of optimisation was carried out using the Code Institute recommended hyperparameter ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model__n_estimators\": [50, 100, 140],\n",
        "        \"model__max_depth\": [None, 4, 15],\n",
        "        \"model__min_samples_split\": [2, 50],\n",
        "        \"model__min_samples_leaf\": [1, 50],\n",
        "        \"model__max_leaf_nodes\": [None, 50],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score, make_scorer\n",
        "\n",
        "\n",
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of the test set was significantly worse than the train set, so the model may be overfitting using these parameters.\n",
        "\n",
        "* Recall on heart disease was 95% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 95% for the train set and 73% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second round of optimisation, the max_features hyperparameter was added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model__n_estimators\": [50, 100, 140],\n",
        "        \"model__max_depth\": [None, 4, 15],\n",
        "        \"model__min_samples_split\": [2, 50],\n",
        "        \"model__min_samples_leaf\": [1, 50],\n",
        "        \"model__max_leaf_nodes\": [None, 50],\n",
        "        \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This time, the model performs similarly on the train and test sets:\n",
        "\n",
        "* Recall on heart disease was 89% for the train set and 88% for the test set.\n",
        "* Precision on no heart disease was 86% for the train set and 81% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the third round, the values for n_estimators and min_samples_leaf were expanded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model__n_estimators\": [50, 140, 300, 500, 800, 1200],\n",
        "        \"model__max_depth\": [None, 4, 15],\n",
        "        \"model__min_samples_split\": [2, 50],\n",
        "        \"model__min_samples_leaf\": [1, 2, 5, 10, 50],\n",
        "        \"model__max_leaf_nodes\": [None, 50],\n",
        "        \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
        "        },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model performed the same as in the previous round:\n",
        "\n",
        "* Recall on heart disease was 89% for the train set and 88% for the test set.\n",
        "* Precision on no heart disease was 86% for the train set and 81% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the fourth round, the min_samples_split values were expanded to see whether any improvement in the test set performance could be found.\n",
        "\n",
        "* The n_estimator range was also reduced, as the previous models had favoured lower numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model__n_estimators\": [50, 140, 300, 500],\n",
        "        \"model__max_depth\": [None, 4, 15],\n",
        "        \"model__min_samples_split\": [2, 5, 10, 15, 50],\n",
        "        \"model__min_samples_leaf\": [1, 2, 5, 10, 50],\n",
        "        \"model__max_leaf_nodes\": [None, 50],\n",
        "        \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
        "        },\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model performed the same as in the previous round:\n",
        "\n",
        "* Recall on heart disease was 89% for the train set and 88% for the test set.\n",
        "* Precision on no heart disease was 86% for the train set and 81% for the test set.\n",
        "\n",
        "As no further improvements were found, these hyperparameter value ranges were chosen as the final set for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = {\"LogisticRegression\": LogisticRegression(random_state=0)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before carrying out the optimisation, it is best to understand what the hyperparameters to investigate mean:\n",
        "\n",
        "* ```penalty``` - Adds a regularisation penalty in order to tune overfitting.\n",
        "* ```C``` - Inverse of regularisation strength, gives more weight to data.\n",
        "* ```tol``` - Tolerance for stopping criteria.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first round of optimisation was carried out using the Code Institute recommended hyperparameter ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"LogisticRegression\":{\n",
        "        \"model__penalty\": [\"l2\", \"l1\", \"elasticnet\", None],\n",
        "        \"model__C\": [2, 1, 0.5],\n",
        "        \"model__tol\": [1e-3, 1e-4, 1e-5],\n",
        "    },\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on both the train and test sets was good.\n",
        "\n",
        "* Recall on heart disease was 84% for the train set and 84% for the test set.\n",
        "* Precision on no heart disease was 84% for the train set and 78% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second round of optimisation, the C values were expanded upon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"LogisticRegression\":{\n",
        "        \"model__penalty\": [\"l2\", \"l1\", \"elasticnet\", None],\n",
        "        \"model__C\": [10, 2, 1.0, 0.5, 0.1],\n",
        "        \"model__tol\": [1e-3, 1e-4, 1e-5],\n",
        "    },\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on both the train and test sets was good.\n",
        "\n",
        "* Recall on heart disease was 84% for the train set and 84% for the test set.\n",
        "* Precision on no heart disease was 84% for the train set and 78% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No further optimisation was carried out using this algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = {\"XGBClassifier\": XGBClassifier(random_state=0)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before carrying out the optimisation, it is best to understand what the hyperparameters to investigate mean:\n",
        "\n",
        "* ```n_estimators``` - Number of gradient boosted trees.\n",
        "* ```max_depth``` - Maximum tree depth.\n",
        "* ```gamma``` - Minimum loss reduction required to split a node further, regularisation factor.\n",
        "* ```learning_rate``` - Applies a weighting factor to corrections by new trees in order to slow down the learning and prevent overfitting.\n",
        "* ```min_child_weight``` - Minimum weight required for a node to split further.\n",
        "* ```subsample``` - The fraction of the dataset sampled for each tree.\n",
        "* ```colsample_bytree``` - The fraction of features used for each tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first round of optimisation was carried out using the Code Institute recommended hyperparameter ranges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"XGBClassifier\":{\n",
        "        \"model__n_estimators\": [30, 80, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.1],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on both the train and test sets was good, with slightly worse performance on the test set.\n",
        "\n",
        "* Recall on heart disease was 86% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 84% for the train set and 72% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the second round of optimisation, min_child_weight was added to determine whether that gave any improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"XGBClassifier\":{\n",
        "        \"model__n_estimators\": [30, 80, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.1],\n",
        "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance was the same as the previous round of optimisation.\n",
        "\n",
        "* Recall on heart disease was 86% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 84% for the train set and 72% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the third round of optimisation, subsample and colsample_bytree hyperparameters were both added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"XGBClassifier\": {\n",
        "        \"model__n_estimators\": [30, 80, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.1],\n",
        "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
        "        \"model__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        \"model__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance in this round was slight worse than the previous round of optimisation.\n",
        "\n",
        "* Recall on heart disease was 84% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 83% for the train set and 72% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " The best parameters chosen between the last two rounds of optimisation was quite different. \n",
        " \n",
        " For the fourth round, the range of values for n_estimators and gamma was increased while keeping other values the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"XGBClassifier\": {\n",
        "        \"model__n_estimators\": [10, 30, 50, 100, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.05, 0.075, 0.1],\n",
        "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
        "        \"model__subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        \"model__colsample_bytree\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance in this round was the same as the previous round of optimisation.\n",
        "\n",
        "* Recall on heart disease was 84% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 83% for the train set and 72% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the fifth round of optimisation, the subsample and colsample_bytree parameters were removed in order to determine whether they were negatively impacting the optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"XGBClassifier\": {\n",
        "        \"model__n_estimators\": [10, 30, 50, 100, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.05, 0.075, 0.1],\n",
        "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
        "        }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "search = HyperparameterOptimizationSearch(models=model, params=params_search)\n",
        "search.fit(X_train, y_train,\n",
        "           scoring =  make_scorer(recall_score, pos_label=1),\n",
        "           n_jobs=-1, cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extensive_grid_search_summary, extensive_grid_search_pipelines = search.score_summary(sort_by='mean_score')\n",
        "\n",
        "best_model = extensive_grid_search_summary.iloc[0,0]\n",
        "best_parameters = extensive_grid_search_pipelines[best_model].best_params_\n",
        "\n",
        "classification_pipeline = extensive_grid_search_pipelines[best_model].best_estimator_\n",
        "\n",
        "best_parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf_performance(X_train=X_train, y_train=y_train,\n",
        "                X_test=X_test, y_test=y_test,\n",
        "                pipeline=classification_pipeline,\n",
        "                label_map=[\"No Heart Disease\", \"Heart Disease\"]\n",
        "                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance on both the train and test sets recovered to the same as the first two rounds of optimisation.\n",
        "\n",
        "* Recall on heart disease was 86% for the train set and 79% for the test set.\n",
        "* Precision on no heart disease was 84% for the train set and 72% for the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As no further improvements were made, these hyperparameter value ranges were used in final testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Several rounds of hyperparameter optimisation were carried out using the RandomForestClassifier, LogisticRegression and XGBClassifier algorithms.\n",
        "\n",
        "After individually testing each algorithm, the hyperparameter search values were compiled to be used in the final hyperparameter optimisation in the Modelling and Evaluation notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "params_search = {\n",
        "    \"RandomForestClassifier\": {\n",
        "        \"model__n_estimators\": [50, 140, 300, 500],\n",
        "        \"model__max_depth\": [None, 4, 15],\n",
        "        \"model__min_samples_split\": [2, 5, 10, 15, 50],\n",
        "        \"model__min_samples_leaf\": [1, 2, 5, 10, 50],\n",
        "        \"model__max_leaf_nodes\": [None, 50],\n",
        "        \"model__max_features\": [None, \"sqrt\", \"log2\"],\n",
        "        },\n",
        "    \"LogisticRegression\":{\n",
        "        \"model__penalty\": [\"l2\", \"l1\", \"elasticnet\", None],\n",
        "        \"model__C\": [10, 2, 1.0, 0.5, 0.1],\n",
        "        \"model__tol\": [1e-3, 1e-4, 1e-5],\n",
        "        },\n",
        "    \"XGBClassifier\": {\n",
        "        \"model__n_estimators\": [10, 30, 50, 100, 200],\n",
        "        \"model__max_depth\": [None, 3, 15],\n",
        "        \"model__learning_rate\": [0.001, 0.01, 0.1],\n",
        "        \"model__gamma\": [0, 0.05, 0.075, 0.1],\n",
        "        \"model__min_child_weight\": [1, 3, 5, 7],\n",
        "        },\n",
        "        }"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
